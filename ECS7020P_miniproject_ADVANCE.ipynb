{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Author\n",
    "**Student Name : Sanjeev Mohan**:  \n",
    "**Student ID : 210990496**:\n",
    "<br>**GitHub Link : https://github.com/mysanjeev99/MLEnd-AudioSegment_Detection**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Problem formulation\n",
    "\n",
    "Using the MLEnd Hums and Whistles dataset, build a machine learning pipeline that takes as an input an audio segment and predicts its song label as well as differentiate wether the song's interpretation is HUM or WHISTLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import All Libraries\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, re, pickle, glob\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import IPython.display as ipd\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version =\",tf.__version__)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download file from online storage\n",
    "\n",
    "def download_url(url, save_path):\n",
    "    with urllib.request.urlopen(url) as dl_file:\n",
    "        with open(save_path, 'wb') as out_file:\n",
    "            out_file.write(dl_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files to local storage\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=6P0I5CAE7JPPKNDU/4P729FD6DFNO8IT4NNFR458\"\n",
    "save_path = './Data/potter_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=452THMOKUOST11V4/4L7IBFL5AJ3AMR45JVV9AC0\"\n",
    "save_path = './Data/potter_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=4HOGB7IH67J8QGL3/5D5TJMIP30DFVC69AQU5ETO\"\n",
    "save_path = './Data/starwars_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=4LUHB7IH67J8QGL3/6T5IF3SK8KQTL7FRD2113DO\"\n",
    "save_path = './Data/starwars_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=6P5DHMOKUOST11V4/45V197K56GJCN375FM97GKO\"\n",
    "save_path = './Data/panther_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=556JF9IP9676AAIB/450D7KAI0S5U15VGE2A7MKO\"\n",
    "save_path = './Data/panther_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=610DVT8AV0VTD6FQ/4PSHB4CI8V5Q0LTGMBBBVO0\"\n",
    "save_path = './Data/rain_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=5163F9QO9A6M8AQ8/6P0T9MQO34CVTCEEA6K4AR8\"\n",
    "save_path = './Data/rain_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=555I92D6GH5I5L9M/59UH97479B4AQOLUKB0QI08\"\n",
    "save_path = './Data/hakuna_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=6TF3DEBMAIDN49A6/6L4DRTRS7KUT51CTGADNEN8\"\n",
    "save_path = './Data/hakuna_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=554DRGJTDMH4I3JF/595DLIDADVHOGJTBM3BRS68\"\n",
    "save_path = './Data/mamma_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=4HVHB72J6VS9UK5D/610DRSJH409EFA60BQVQQ20\"\n",
    "save_path = './Data/mamma_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=5942PCQC7ROPGNTS/6PF33CLHC39AEQ41JJS9AC8\"\n",
    "save_path = './Data/showman_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=6132RDI977V9AMLR/6L7J7ETOF7SPSKDEMB5Q81G\"\n",
    "save_path = './Data/showman_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=4LV194DEJH9JLV8U/652DJI5BDRM82DIIPKKCFQ0\"\n",
    "save_path = './Data/frozen_1.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)\n",
    "\n",
    "\n",
    "url  = \"https://collect.qmul.ac.uk/down?t=6TDILDI977V9AMLR/6P4TPGT0A70LC6BHTK7VPMG\"\n",
    "save_path = './Data/frozen_2.zip'\n",
    "path = Path(save_path)\n",
    "if path.is_file():\n",
    "    print(path,' exist')\n",
    "else:\n",
    "    download_url(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the Directory\n",
    "path = '.\\Data'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio files from zip to folder\n",
    "\n",
    "directory_to_extract_to = './Data/AudioData/'\n",
    "zip_path = './Data/potter_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/potter_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/starwars_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/starwars_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/panther_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/panther_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/rain_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/rain_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/hakuna_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/hakuna_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "    \n",
    "zip_path = './Data/mamma_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/mamma_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/showman_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/showman_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/frozen_1.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)\n",
    "\n",
    "zip_path = './Data/frozen_2.zip'\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Machine Learning pipeline\n",
    "<br>Stage 1 : Clean Data to remove inconsistant file naming.\n",
    "<br>Stage 2 : Transfer consistant data to different folder.\n",
    "<br>Stage 3 : Create Table containing attributes of the song files in the consistant data folder.\n",
    "<br>Stage 4 : Create a Dataframe of the Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "# Cleaning the data for inconsistant\n",
    "\n",
    "# Change to upper case and Remove space in file\n",
    "import os\n",
    "path = '.\\\\Data\\\\AudioData\\\\'\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    os.rename(path + file, path + file.upper())\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\" \",\"\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f,path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"[\",\"\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f,path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"]\",\"\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f,path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"(\",\"\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f,path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\")\",\"\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f,path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\".WAV.WAV\",\".WAV\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "#Data Cleansing and Correction\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"HARRYPOTTER\",\"POTTER\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"HARRY_POTTER_THEME_SONG\",\"POTTER\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"STAR_WARS\",\"STARWARS\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"IMPERIAL_MARCH.WAV\",\"STARWARS.WAV\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"IMPERIAL.WAV\",\"STARWARS.WAV\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"PINK_PANTHER.WAV\",\"PANTHER.WAV\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"HAKUNAMATATA.WAV\",\"HAKUNA.WAV\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM1_\",\"_HUM_1_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM2_\",\"_HUM_2_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM3_\",\"_HUM_3_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM4_\",\"_HUM_4_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM1\",\"_HUM_1_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM2\",\"_HUM_2_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM3\",\"_HUM_3_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_HUM4\",\"_HUM_4_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE1_\",\"_WHISTLE_1_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE2_\",\"_WHISTLE_2_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE3_\",\"_WHISTLE_3_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE4_\",\"_WHISTLE_4_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE1\",\"_WHISTLE_1_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE2\",\"_WHISTLE_2_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE3\",\"_WHISTLE_3_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)\n",
    "\n",
    "for f in os.listdir(path):\n",
    "    r = f.replace(\"_WHISTLE4\",\"_WHISTLE_4_\")\n",
    "    if( r != f):\n",
    "        os.rename(path + f, path + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2\n",
    "# Transfer consistant data to different Folder\n",
    "\n",
    "destination = '.\\\\Data\\\\FilteredAudioData\\\\' \n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'POTTER.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'POTTER.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'STARWARS.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'STARWARS.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'PANTHER.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'PANTHER.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'RAIN.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'RAIN.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'HAKUNA.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'HAKUNA.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'MAMMA.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'MAMMA.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'SHOWMAN.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'SHOWMAN.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'HUM' and (file.split('\\\\')[-1].split('_')[3]) == 'FROZEN.WAV':\n",
    "        os.replace(path + file, destination + file)\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    if (file.split('\\\\')[-1].split('_')[1]) == 'WHISTLE' and (file.split('\\\\')[-1].split('_')[3]) == 'FROZEN.WAV':\n",
    "        os.replace(path + file, destination + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of consistant data \n",
    "filtered = '.\\\\Data\\\\FilteredAudioData\\\\*.wav'\n",
    "files = glob.glob(filtered)\n",
    "print(\"Total Consistant Data in the folder = \",len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3\n",
    "# Create Table containing attributes of the song file in the consistant data folder\n",
    "\n",
    "MLENDHW_table = [] \n",
    "\n",
    "for file in files:\n",
    "  file_name = file.split('\\\\')[-1]\n",
    "  participant_ID = file.split('\\\\')[-1].split('_')[0]\n",
    "  interpretation_type = file.split('\\\\')[-1].split('_')[1]\n",
    "  interpretation_number = file.split('\\\\')[-1].split('_')[2]\n",
    "  song = file.split('\\\\')[-1].split('_')[3].split('.')[0]+'_'+file.split('\\\\')[-1].split('_')[1]\n",
    "  MLENDHW_table.append([file_name,participant_ID,interpretation_type,interpretation_number, song])\n",
    "\n",
    "MLENDHW_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4\n",
    "# Create a Dataframe of the Table\n",
    "MLENDHW_df = pd.DataFrame(MLENDHW_table,columns=['file_id','participant','interpretation','number','song'])\n",
    "MLENDHW_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Transformation stage\n",
    "\n",
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract feature\n",
    "def features_extractor(file):\n",
    "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=80)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "       \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Feature and append to a list\n",
    "extracted_features=[]\n",
    "AudioLoc = '.\\\\Data\\\\FilteredAudioData\\\\'   \n",
    "\n",
    "for index, filedata in tqdm(MLENDHW_df.iterrows()):\n",
    "  class_labels = filedata[\"song\"]\n",
    "  file_name = os.path.join(AudioLoc + filedata[\"file_id\"])\n",
    "\n",
    "  features = features_extractor(file_name)\n",
    "  extracted_features.append([features, class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting extracted_features to Pandas dataframe\n",
    "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the data\n",
    "print(\"Total data under Label\")\n",
    "print(\"\")\n",
    "print(extracted_features_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X array from dataframe['feature'] and y array from dataframe['class']\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using fit_transform\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about X and y array\n",
    "print(\"Shape of y is \",y.shape)\n",
    "print(\"Shape of X is \",X.shape)\n",
    "print(\"\")\n",
    "print(\"Total Label = \",y.shape[1])\n",
    "print(\"Total Dataset = \",y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset to train data and test data\n",
    "size=0.3\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=size,random_state=0)\n",
    "print(\"Splitting Dataset to Training and Testing Data on RATIO : \",size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Information about Training data and test data\n",
    "print(\"Total Training Data = \",X_train.shape[0])\n",
    "print(\"Total Testing Data = \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No of classes\n",
    "num_labels=y.shape[1]\n",
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modelling\n",
    "\n",
    "keras.Sequential model - multiple layes with activation is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cleare existing keras backend if any\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras Sequential Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(3243, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(1621, activation='relu'))\n",
    "model.add(Dense(810, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(405, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(202, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(101, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels, activation=tf.nn.sigmoid))\n",
    "# Final Layer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Print Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Methodology\n",
    "\n",
    "the model will be train using tf.keras, The model performance is assesssed based on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H-%M-%S\", t)\n",
    "\n",
    "## Trianing my model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 300\n",
    "num_batch_size = 32\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=f'saved_models/audio_classification_{current_time}.hdf5',save_best_only=True, verbose=1)\n",
    "start = datetime.now()\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model\n",
    "loss, acc = model.evaluate(X_train,y_train,verbose=2)\n",
    "print('accuracy: {:5.2f}%'.format(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Dataset\n",
    "\n",
    "\n",
    "Dataset contains Audio Segmens with respect to their interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Potter_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Rain_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Frozen_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=4\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Hakuna_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Mamma_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=6\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Panther_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=9\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Showman_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "print(files[n])\n",
    "fs = None # Sampling frequency. If None, fs would be 22050\n",
    "x, fs = librosa.load(files[n],sr=fs)\n",
    "t = np.arange(len(x))/fs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Starwars_HUM Spectogram Sample {n}',size=15)\n",
    "plt.plot(t,x,c='blue')\n",
    "plt.xlabel('time (sec)')\n",
    "plt.ylabel('amplitude')\n",
    "plt.show()\n",
    "display(ipd.Audio(files[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print corresponding label from above prediction\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label)\n",
    "print(prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting X_test, the output ranges 0 to 15 which corresponds to the label\n",
    "predicted_label=model.predict_classes(X_test)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Results\n",
    "\n",
    "Carry out your experiments here, explain your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model Training Graph\n",
    "pd.DataFrame(history.history).plot(figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict name of the song and wether it is HUM or WHISTLE from audio file\n",
    "\n",
    "filename=\".\\\\Data\\\\AudioData\\\\S5_HUM_3_MUMMAMIA.WAV\"\n",
    "print(\"File = \",filename)\n",
    "display(ipd.Audio(filename, autoplay=False))\n",
    "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=80)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "\n",
    "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
    "predicted_label=model.predict_classes(mfccs_scaled_features)\n",
    "#print(predicted_label)\n",
    "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
    "print(\"Predicted Song = \",prediction_class[0].split('_')[0])\n",
    "print(\"\\nPredicted Interpretation = \",prediction_class[0].split('_')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Conclusions\n",
    "\n",
    "The Trained model is accuracy: 71.54%\n",
    "<br>This model can predict the Song Name of the Audio Segment and as well Audio Segment's Interpretation.\n",
    "<br>The training accuracy can be further increased by fine tuining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
